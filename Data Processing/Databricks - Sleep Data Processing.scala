// Databricks notebook source
// MAGIC %md
// MAGIC ## CS598: Automated Sleep Stage Scoring
// MAGIC #### In this notebook we demonstrate a genearlized approach to collecting raw sensor and scoring data from PSG/Hypnogram files and generating a flexible data model suitable for experimenting with sleep stage scoring based on 30-second epochs of time
// MAGIC This single notebook can be used for automating a processing pipeline, but the cells are also structured in a manner to allow exploratory analysis of the data throughout the processing stages<br/>
// MAGIC <b><i>Note that multiple "display" calls are included to show the data as it moves through the pipeline for documentation purposes.  These should be removed in a production setting to avoid repeatedly materializing the data.

// COMMAND ----------

import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

// COMMAND ----------

// MAGIC %md
// MAGIC Browse the input file system

// COMMAND ----------

display(dbutils.fs.ls("abfss://datalake@bigdatatechnologiesdl.dfs.core.windows.net/sleep_cassette/input/"))

// COMMAND ----------

// MAGIC %md
// MAGIC Read in the score files.  The expected input is a series of CSV files, generated by Polymon where each file represents the entire set of annotations for a single sample based on the naming convention "<i>SUBJECTID</i>_SCORE.CSV"
// MAGIC 
// MAGIC Example input:<br/>
// MAGIC <code>
// MAGIC Date,Time,onset,duration,description,Linked Channel<br/>
// MAGIC 1989-04-25,00:13:30,0,1800,Sleep stage W, <br/>
// MAGIC 1989-04-25,00:43:30,0,120,Sleep stage 1, <br/>
// MAGIC 1989-04-25,00:45:30,0,390,Sleep stage 2, <br/>
// MAGIC 1989-04-25,00:52:00,0,30,Sleep stage 3, <br/>
// MAGIC 1989-04-25,00:52:30,0,30,Sleep stage 2, <br/>
// MAGIC </code>

// COMMAND ----------

// use a custom schema to override the column name "time" as that is a reserved keyword
val customSchema = StructType(Array(
  StructField("date", StringType, false),
  StructField("secs", StringType, false),
  StructField("onset", StringType, false),
  StructField("duration", StringType, false),
  StructField("annotation", StringType, false),
  StructField("linkedChannel", StringType, false))
)
// subject is not a part of the input itself, use file naming convention to extract subject ID
val scores = spark.read.option("header",true).schema(customSchema).csv("abfss://datalake@bigdatatechnologiesdl.dfs.core.windows.net/sleep_cassette/input/SC4*_SCORE.csv")
  .withColumn("filepath",input_file_name)
  .withColumn("filename",reverse(split(col("filepath"),"/")).getItem(0))
  .withColumn("subject",split(col("filename"), "_").getItem(0))
scores.cache()
display(scores.orderBy($"subject",$"onset"))

// COMMAND ----------

// MAGIC %md
// MAGIC Strip off the extraneous text "Sleep stage" and just keep the annotation value

// COMMAND ----------

val scores_annotated = scores.withColumn("aa",split(col("annotation")," ").getItem(2)).drop("annotation").withColumnRenamed("aa", "annotation")
display(scores_annotated)

// COMMAND ----------

// MAGIC %md
// MAGIC This next series of cells uses the start time and duration values to generate a range of timestamps that correspond to each 30-second epoch.  Note: the "between" operation is inclusive (i.e. <= and >=) so we need our end time to stop short of the following epoch start time

// COMMAND ----------

spark.conf.set("spark.sql.legacy.timeParserPolicy","LEGACY")
val scores_with_time = scores_annotated.withColumn("timestamp_begin", to_timestamp(concat(col("date"),lit(' '),col("secs")), "yyyy-MM-dd HH:mm:ss"))
//display(scores_with_time)

// COMMAND ----------

val scores_with_time_range = scores_with_time.withColumn("timestamp_end", expr("timestamp_begin + make_interval(0, 0, 0, 0, 0, 0, duration)"))
//display(scores_with_time_range)

// COMMAND ----------

val scores_with_time_range_corrected = scores_with_time_range.withColumn("timestamp_end_excl",col("timestamp_end") - expr("INTERVAL .01 SECONDS"))
display(scores_with_time_range_corrected.orderBy($"subject", $"timestamp_begin"))

// COMMAND ----------

// MAGIC %md
// MAGIC Read in the sensor data files.  The expected input is a series of Parquet files, containing data exported by Polymon where each file represents the entire set of all channels of sensor data for a single sample based on the naming convention "<i>SUBJECTID</i>_100Hz.parquet"
// MAGIC 
// MAGIC Example input:<br/>
// MAGIC <code>
// MAGIC Date	HH	MM	SS	EEG Fpz-Cz[uV]	EEG Pz-Oz[uV]	EOG horizontal[uV] <br/>
// MAGIC 1989-07-21	0	59	59.99	-3.25128205128206	1.87619047619049	0.746520146520207<br/>
// MAGIC 1989-07-21	1	0	0	10.2923076923077	-0.625396825396809	-0.248840048839989<br/>
// MAGIC 1989-07-21	1	0	0.01	16.2358974358974	0.817826617826634	-0.248840048839989<br/>
// MAGIC 1989-07-21	1	0	0.02	11.2666666666667	-0.432967032967016	2.2395604395605<br/>
// MAGIC 1989-07-21	1	0	0.03	4.15384615384615	0.0481074481074647	-1.24420024420018<br/>
// MAGIC 1989-07-21	1	0	0.04	3.76410256410256	-0.33675213675212	1.7418803418804<br/>
// MAGIC </code>
// MAGIC Some minor cleanup of column headers is done, as well as extracting the subject from the filename, similar to the score data.<br/>
// MAGIC Additionally, the time values are split up over several columns so those are combined and a timestamp field is generated

// COMMAND ----------

val data = spark.read.parquet("abfss://datalake@bigdatatechnologiesdl.dfs.core.windows.net/sleep_cassette/input/SC4*_100Hz.parquet")
//val data = spark.read.parquet("dbfs:/jkolter/sleep/auto_input/SC4*_100Hz.parquet")
  .withColumn("filepath",input_file_name)
  .withColumn("filename",reverse(split(col("filepath"),"/")).getItem(0))
  .withColumn("subject2",split(col("filename"), "_").getItem(0))
val data_trun_seconds = data.withColumn("ss",split(col("SS"),"\\.").getItem(0).cast(IntegerType))
val data_floor_secs = data_trun_seconds.withColumn("epoch_seconds", floor(col("ss")/30)*30)
val data_with_time = data_floor_secs.withColumn("timestamp", to_timestamp(concat(col("Date"),lit(' '),col("HH"),lit(':'),col("MM"),lit(':'),col("epoch_seconds")), "yyyy-MM-dd HH:mm:ss"))
data_with_time.cache
display(data_with_time)

// COMMAND ----------

// MAGIC %md
// MAGIC Join the sensor readings with the scores to match the annotation to the sensor value at that time. The join is keyed on subject id, and the sensor reading timestamp being contained within a single epoch

// COMMAND ----------

val df = data_with_time.join(
  scores_with_time_range_corrected,
  data_with_time("subject2") <=> scores_with_time_range_corrected("subject") 
    && data_with_time("timestamp").between(scores_with_time_range_corrected("timestamp_begin"),scores_with_time_range_corrected("timestamp_end_excl")), 
  "inner")
display(df)   

// COMMAND ----------

// MAGIC %md
// MAGIC The next series of cells creates the 3 vectors which correspond to each channel

// COMMAND ----------

val big_group = df.groupBy("subject", "timestamp", "annotation").agg(collect_list("EEG_Fpz-Cz_uV").as("EEG_Fpz-Cz_uV"))

// COMMAND ----------

val big_group2 = df.groupBy("subject", "timestamp", "annotation").agg(collect_list("EEG_Pz-Oz_uV").as("EEG_Pz-Oz_uV"))

// COMMAND ----------

val big_group3 = df.groupBy("subject", "timestamp", "annotation").agg(collect_list("EOG_horizontal_uV").as("EOG_horizontal_uV"))

// COMMAND ----------

// MAGIC %md
// MAGIC Combine the 3 vectors into a single row per subject/epoch annotation key, ordered by timestamp ascending to create the final output dataframe<br/>
// MAGIC <b><i>Note: do not try to collect the results onto the driver using "display" or "collect" or similar, the volume is too large

// COMMAND ----------

val final_dataset = big_group.join(big_group2,Seq("subject", "timestamp", "annotation")).join(big_group3,Seq("subject", "timestamp", "annotation")).orderBy("subject","timestamp")

// COMMAND ----------

// MAGIC %md
// MAGIC Save the final dataframe in parquet format

// COMMAND ----------

final_dataset.repartition($"subject").write.partitionBy("subject").parquet("abfss://datalake@bigdatatechnologiesdl.dfs.core.windows.net/sleep_cassette/output_full_partitioned")

// COMMAND ----------

// MAGIC %md
// MAGIC Browse the output file system

// COMMAND ----------

display(dbutils.fs.ls("abfss://datalake@bigdatatechnologiesdl.dfs.core.windows.net/sleep_cassette/output_full_partitioned/"))
